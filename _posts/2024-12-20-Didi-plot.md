---
layout: post
title:  "Didi Plots"
date:   2024-12-06 12:57:48 +0000
categories: jekyll update
---

Perhaps one of things I'm most proud about is being able to shoehorn my daughters name into a publication.

Here to be precise:

![didiP_kmex](/assets/didiPlot_fig1.png)

This is an extract from a paper that was published back in 2022 with the rather cumbersome title of "Kernel hazard estimation for visualisation of the effect of a continuous covariate on time‐to‐event endpoints" [1].

But I'm getting ahead of myself. Best start again.

## The Problem

We were interested in understanding how biomarkers may help explain survival in patients with pancreatic cancer.  The problem was that the biomarkers we were looking at were all continuous.  

The standard way to start is to plot a Kaplan Meier graph.  But this can't be done with continuous covariates and so we are forced to create groups.    But we know this is bad (it's been well stablished [2]) but aside from the loss of information and the potential to mis-classify patients; grouping continuous just isn't quite good enough if you want to understand the true underlying relationship.

Lets take an example;  CA19.9 is a biomarker that is measured after patients have undergone surgery but before their post-therapy chemotherapy begins.    Our outcome, Overall survival measured from the point of surgery. Patients were censored either if they withdrew consent from the study or if they were alive at the point at which the study ended.

Now, despite creating their problems, clinicians do love a category and values of 37, 90 and 180 can be used [refernce].  As far as we could see however, the only redeeming feature of categorisation was the ability to make Kaplan Meier graphs. This is what they look like:



![didiP_kmex](/assets/didiPlot_fig2.png)


And well, we thought we could do better than this.

## The Solution


Immediately we knew that adjusting the Kaplan Meier estimate was not going to get us what we needed and so we had two problems to solve:
1. Finding another way to estimate a survival function
2. Allowing this to be adjusted in a way which lets us see the impact of a continuous covariate

### Problem 1: (Re)estimating a survival function

We thought about fitting some type of parametric model to our data but we abandoned that early on - essentially we wanted to see the raw data - not the predicted output of a model.  

Previously my boss had been interested in Effrons' methods of re-weighting to create survival estimates and we decided to look a bit further.  This is a simple idea and follows; if we have (say) 5 patients, 4 who have died and 1 who is censored.  We can think of their data as something like this:

| time | censoring |
| ---- | --------- |
| 12   | 1         |
| 14   | 1         |
| 19   | 0         |
| 26   | 1         |
| 28   | 1         |

Here the patient with a survival time of 19 is censored.  Effron' weighting method began by giving every patient a weight of 1....

| time | censoring | E. weight |
| ---- | --------- | --------- |
| 12   | 1         | 1         |
| 14   | 1         | 1         |
| 19   | 0         | 1         |
| 26   | 1         | 1         |
| 28   | 1         | 1         |

The basic idea is that we can only evaluate the survival function at the observed event times and that the third patient should be removed from the dataset.  Upon removing this patient however, their weight needs to be re-distributed across all the patients who are still alive at this point in time.  Now patient 19 had a weight of '1' and we need to distribute this across 2 patients alive and so we give 1/2 weight to each remaining patient. Our data now look like this

| time | weight |
| ---- | ------ |
| 12   | 1      |
| 14   | 1      |
| 26   | 1.5    |
| 28   | 1.5    |

As it turns out removing censoring in this way is incredibly useful.  We can now estimate the distribution of time using more standard approached.  We use a kernel density estimator.  Without going into too much detail this is a handy way to build a density by adding a 'kernel' to each individual data point.  Once we have done this we can add up all of these individual distributions and voila - we can see what our density function looks like.  Posts like this can explain this much better than I ever can (https://ekamperi.github.io/math/2020/12/08/kernel-density-estimation.html)

Lets call the kerne, for each patient $K(t_i)$.  We apply our weights, which we call $d_i$ and then for each patient we have $d_i K(t_i)$.  This means that for any random point in time we can sum over all the kernels in our dataset and we will get an estimate of the distribution of survival times

$$
	\hat{f(t)} = \sum_{i=1}^n d_i K(t_i)
$$

### Problem 2:  Adjusting for a continuous covariate.

One of the nice things about the weighted kernel estimator is it allows us to estimate a density function at any point in time - not just those where events are observed.

Now - how to incorporate of covariate.  Lets call it x as seems sensible.   Our aim is that for any value of our covariate we can use the weighted kernel estimator to get a survival function for that particular value.

Say for example we wanted to understand the survival for a patient with a covariate value of x=4.  The basic idea is that patients who have a value close to 4 should contribute a lot to this estimate and that those who have a value far away from 4 should only contribute a little (if at all).

Lets go back to our data and add in some covariate data

| time | censoring | E. weight $(d_i)$ | x   |
| ---- | --------- | ----------------- | --- |
| 12   | 1         | 1                 | 6   |
| 14   | 1         | 1                 | 3   |
| 19   | 0         | 1                 | 5   |
| 26   | 1         | 1                 | 9   |
| 28   | 1         | 1                 | 1   |

Now if we are looking at a survival function associated with a value of x=4 we may apply a second set of weights which look something like this....

| time | censoring | E weight $(d_i)$ | x   | x weight |
| ---- | --------- | ---------------- | --- | -------- |
| 12   | 1         | 1                | 6   | 0.5      |
| 14   | 1         | 1                | 3   | 0.8      |
| 19   | 0         | 1                | 5   | 0.8      |
| 26   | 1         | 1                | 9   | 0.2      |
| 28   | 1         | 1                | 1   | 0.3      |

For reasons which will later become clear we label this second weight as $\delta_i$. The point is that we can combine the new weights with the Effron weights ($d_i$)...

| time | censoring | comb weight $(d_i \delta_i)$ |
| ---- | --------- | ---------------------------- |
| 12   | 1         | 0.5                          |
| 14   | 1         | 0.8                          |
| 19   | 0         | 0.8                          |
| 26   | 1         | 0.2                          |
| 28   | 1         | 0.3                          |

When we apply apply Effrons's weights we now get a dataset like above which can be used to create a survival function.  The new datasets looks like this.  Remember the censored patient who was removed had a weight of 0.8 and so we allocated 0.4 to each of the two remaining patients

| time | comb weight $(d_i \delta_i)$ |
| ---- | ---------------------------- |
| 12   | 0.5                          |
| 14   | 0.8                          |
| 26   | 0.6                          |
| 28   | 0.7                          |

Importantly though - this will produce a survival function specifically for a value of x=4.  If we chose a different value (x=6 say) we would get a different set of 'x weights' ($\delta_i$) and a different survival function.

What we end up with is something that looks like below for CA19.9 and hopefully you can see that instead of categorising patients we can now estimate survival functions which are relevant for each individual patient...

![didiP_kmex](/assets/didiPlot_fig3.png)

Going one step further however, it'd be nice to see what the survival function is for *all* the values of our covariate.  This is what we do but to show this we will need a contour plot...

![didiP_kmex](/assets/didiPlot_fig4.png)

So I hope you agree this looks much nicer than a Kaplan Meier Graph.   More importantly we can see that Survival is best around values of CA19.9 = 2 and then slopes down in a more or less continuous fashion.    Median survival occurs where the colours change and so ranges from around 30 months for patients with a value close to 2 to less than 10 months for patients with a CA19.9 >7

Importantly it doesn't seem here like there is any obvious place for us to create groups.  You may argue that all patients above a value of 6 say have a uniform survival but for those below 6 there is a huge variability in their survival outlook.

## Using the Didi plot

So 2 years after publication, I have finally go around to creating an R package which hopefully makes this all a bit easier for you.

Once you've installed and laoded the package:

```
install.packages("didi")
library(didi)
```

the main function you will need is the 'didiEst()' function.  This will require as a minimum

	st: Survival Time data
	cen: Censoring Data
	x: A continuous covariate

You can also set band width parameters for the kernels (bw) and the covairate weights (bw.x).  These control the 'smoothness' of the graph you get.

To find out a little more you can visit the development site at https://github.com/richJJackson/didi

Next I have to find a way to get my son's name involved somehow.... more about that to come later.





### Reference
Jackson RJ, Cox TF. Kernel hazard estimation for visualisation of the effect of a continuous covariate on time‐to‐event endpoints. Pharmaceutical Statistics. 2022 May;21(3):514-24.

Altman DG, Lausen B, Sauerbrei W, Schumacher M.  Dangers of Using “Optimal” Cutpoints in the Evaluation of Prognostic Factors. Vol  86. Oxford University Press; 1994: 829-835.


